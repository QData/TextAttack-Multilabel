{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with TextAttack-Multilabel\n",
    "\n",
    "This notebook demonstrates how to use TextAttack-Multilabel to generate adversarial examples for multi-label toxicity classification.\n",
    "\n",
    "## Installation\n",
    "\n",
    "First, install the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install from source (or pip install textattack-multilabel)\n",
    "# !pip install -e .\n",
    "\n",
    "# Install dependencies\n",
    "!python scripts/install_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "Let's start with a simple example using the pre-configured setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from textattack_multilabel import MultilabelModelWrapper\n",
    "from textattack_multilabel.multilabel_target_attack import MultilabelACL23\n",
    "from detoxify import Detoxify\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model\n",
    "\n",
    "Let's load a pre-trained toxicity detection model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Detoxify model for toxicity detection\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "detoxify_model = Detoxify('original', device=device)\n",
    "\n",
    "# Create the model wrapper\n",
    "class DetoxifyWrapper(MultilabelModelWrapper):\n",
    "    def __init__(self, detoxify_model, device):\n",
    "        from transformers import RobertaTokenizer\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('FacebookAI/roberta-base')\n",
    "        tokenizer.model_max_length = 128\n",
    "        super().__init__(detoxify_model.model, tokenizer, multilabel=True, device=device, max_length=128)\n",
    "        self.detoxify = detoxify_model\n",
    "    \n",
    "    def __call__(self, text_input_list):\n",
    "        predictions = self.detoxify.predict(text_input_list)\n",
    "        if isinstance(predictions, dict):\n",
    "            import numpy as np\n",
    "            pred_array = np.stack([predictions[label] for label in self.detoxify.class_names])\n",
    "            return torch.tensor(pred_array.T, dtype=torch.float32)\n",
    "        return predictions\n",
    "\n",
    "model_wrapper = DetoxifyWrapper(detoxify_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "Let's test our model on some sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "test_texts = [\n",
    "    \"This is a normal comment.\",\n",
    "    \"You are an amazing person!\",\n",
    "    \"This product is great, I highly recommend it.\",\n",
    "    \"You are stupid and worthless.\"\n",
    "]\n",
    "\n",
    "predictions = model_wrapper(test_texts)\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "print(\"Label names:\", detoxify_model.class_names)\n",
    "print(\"\\nPredictions:\")\n",
    "for text, pred in zip(test_texts, predictions):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Toxicity scores: {pred.tolist()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Attack Recipe\n",
    "\n",
    "Now let's create an attack recipe to generate adversarial examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attack recipe for attacking benign samples (making them appear toxic)\n",
    "attack = MultilabelACL23.build(\n",
    "    model_wrapper=model_wrapper,\n",
    "    labels_to_maximize=[0, 1, 2, 3, 4, 5],  # Maximize all toxic labels\n",
    "    labels_to_minimize=[],\n",
    "    maximize_target_score=0.8,\n",
    "    minimize_target_score=0.2,\n",
    "    wir_method=\"delete\",  # Word Importance Ranking method\n",
    "    pos_constraint=True,   # Use part-of-speech constraints\n",
    "    sbert_constraint=False # Skip SBERT for speed in demo\n",
    ")\n",
    "\n",
    "print(\"Attack recipe created successfully!\")\n",
    "print(f\"Goal function: {attack.goal_function}\")\n",
    "print(f\"Search method: {attack.search_method}\")\n",
    "print(f\"Number of constraints: {len(attack.constraints)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Attack\n",
    "\n",
    "Let's attack a single benign example to create an adversarial example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with one example\n",
    "import textattack\n",
    "\n",
    "# Our benign example\n",
    "original_text = \"This movie was excellent and very well made.\"\n",
    "ground_truth_labels = [0.1, 0.05, 0.02, 0.01, 0.03, 0.02]  # Very low toxicity scores\n",
    "\n",
    "# Create TextAttack dataset\n",
    "dataset = textattack.datasets.Dataset([(original_text, ground_truth_labels)])\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(original_text)\n",
    "print(f\"Original scores: {ground_truth_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the attack\n",
    "attacker = textattack.Attacker(attack, dataset)\n",
    "attack_results = attacker.attack_dataset()\n",
    "\n",
    "# Display results\n",
    "for result in attack_results:\n",
    "    print(\"\\n--- Attack Result ---\")\n",
    "    print(f\"Attack successful: {result.perturbed_result.goal_status == textattack.goal_function_results.GoalFunctionResultStatus.SUCCEEDED}\")\n",
    "    print(f\"Original text: {result.original_result.attacked_text.text}\")\n",
    "    print(f\"Adversarial text: {result.perturbed_result.attacked_text.text}\")\n",
    "    print(f\"Original scores: {result.original_result.output}\")\n",
    "    print(f\"Adversarial scores: {result.perturbed_result.output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Configuration\n",
    "\n",
    "For more complex attacks, you can use the configuration file approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This demonstrates how to use the modular script approach\n",
    "# The actual script is in scripts/attack_multilabel.py\n",
    "\n",
    "print(\"For larger scale attacks, use the script approach:\")\n",
    "print(\"python scripts/attack_multilabel.py --config config/attack_config.yaml --attack benign\")\n",
    "\n",
    "# Available configuration options:\n",
    "print(\"\\nConfiguration includes:\")\n",
    "print(\"- Model selection (detoxify variants or custom HF models)\")\n",
    "print(\"- Dataset configuration (Jigsaw or custom CSV)\")\n",
    "print(\"- Attack parameters (constraints, transformations, search methods)\")\n",
    "print(\"- Output settings (format, location, metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before running attacks, you may want to preprocess your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example preprocessing commands:\n",
    "print(\"Data preprocessing examples:\")\n",
    "print(\"# Analyze dataset\")\n",
    "print(\"python scripts/preprocess_data.py --data data/jigsaw_toxic_comments/test.csv --analyze\")\n",
    "print()\n",
    "print(\"# Sample benign examples for attack\")\n",
    "print(\"python scripts/preprocess_data.py --data data/jigsaw_toxic_comments/test.csv --sample benign\")\n",
    "print()\n",
    "print(\"# Run full pipeline: download -> preprocess -> attack\")\n",
    "print(\"python scripts/download_data.py\")\n",
    "print(\"python scripts/preprocess_data.py --data data/jigsaw_toxic_comments/test.csv --sample benign\")\n",
    "print(\"python scripts/attack_multilabel.py --config config/attack_config.yaml --attack benign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tests\n",
    "\n",
    "Make sure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests\n",
    "!python scripts/run_tests.py --unit-only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
